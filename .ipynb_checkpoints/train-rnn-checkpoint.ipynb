{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from lstm import LSTM\n",
    "from music_dataloader import create_split_loaders\n",
    "from torch_utils import setup_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computing_device = setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, dictionary = create_split_loaders(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    return torch.mean(torch.tensor([criterion(torch.squeeze(model(torch.unsqueeze(x.to(computing_device), 0))), y.to(computing_device)) \\\n",
    "                       for x, y in loader]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn(model, criterion, optimizer, train_loader, val_loader, n_epochs, model_name, seq_length=100, \n",
    "            chkpt_every=100, update_hist=50, val_every=1000):\n",
    "    train_losses = dict()\n",
    "    val_losses = dict()\n",
    "    total_seen = 0\n",
    "    start_time = dt.now()\n",
    "    \n",
    "    # Make the directory to save the model and the losses\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    if not os.path.exists('train-stats'):\n",
    "        os.mkdir('train-stats')\n",
    "    model_save_path = os.path.join('models', model_name + '.pt')\n",
    "    train_save_path = os.path.join('train-stats', model_name + '_train.pkl')\n",
    "    val_save_path = os.path.join('train-stats', model_name + '_val.pkl')\n",
    "    \n",
    "    for epoch in np.arange(n_epochs):\n",
    "        train_losses[epoch] = []\n",
    "        val_losses[epoch] = []\n",
    "        model.train()\n",
    "        model.reset_state()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(computing_device)\n",
    "            y = y.to(computing_device)\n",
    "            loss = criterion(torch.squeeze(model(torch.unsqueeze(x, 0))), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Question about efficiency with the detachment every iteration\n",
    "            train_losses[epoch].append(loss.detach().cpu().numpy())\n",
    "            #print(type(loss.data.tolist()))\n",
    "            #print(train_losses[0][0])\n",
    "            total_seen += 1\n",
    "            \n",
    "            # Report training stats\n",
    "            avg_loss = np.mean(train_losses[epoch][-update_hist:])\n",
    "            time_delta = dt.now() - start_time\n",
    "            update_str = '[TIME ELAPSED]: {0} [EPOCH {1}]: Avg. loss for last {2} minibatches: {3:0.5f}'\n",
    "            print(update_str.format(str(time_delta),epoch + 1, chkpt_every, avg_loss), end='\\r')\n",
    "            \n",
    "            # Save the model and the training and validation losses\n",
    "            if not total_seen % chkpt_every:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                with open(train_save_path, 'wb') as f:\n",
    "                    pickle.dump(train_losses, f)\n",
    "                with open(val_save_path, 'wb') as f:\n",
    "                    pickle.dump(val_losses, f)\n",
    "            if not total_seen % val_every:\n",
    "                # Validate the model\n",
    "                val_losses[epoch].append(evaluate_model(model, val_loader, criterion))\n",
    "                model.train()\n",
    "        print('')\n",
    "        \n",
    "        val_losses[epoch] = evaluate_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(len(dictionary), 150, len(dictionary), computing_device, 1)\n",
    "criterion = nn.CrossEntropyLoss().to(computing_device)\n",
    "optimizer = Adam(lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rnn(lstm, criterion, optimizer, train_loader, val_loader, 100, 'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tune(model):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
